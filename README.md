# Multilingual GPT-BERT Baseline for BabyBabelLM (Jumelet et al f.c.)

As part of the BabyBabelLM (Multilingual BabyLM) team, we develop Multilingual BabyLM corpora for several languages. 

This is a GPT-BERT multilingual baseline architecture for these datasets. 

I modify the pretraining code provided by Charpentier et al (2024)
